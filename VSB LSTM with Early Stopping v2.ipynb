{
  "cells": [
    {
      "metadata": {
        "_uuid": "e154a47bf09b8770980486e87786317a1b3038e1"
      },
      "cell_type": "markdown",
      "source": "Inspired from Bruno Aquino's Kernel:\nhttps://www.kaggle.com/braquino/5-fold-lstm-attention-fully-commented-0-694\n\nI am using early stopping on the 5 Folds Stratified K Folds."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport pyarrow.parquet as pq # Used to read the data\nimport os \nimport numpy as np\nfrom keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes\nfrom keras.models import Model\nfrom tqdm import tqdm # Processing time measurement\nfrom sklearn.model_selection import train_test_split \nfrom keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class\nfrom keras import optimizers # Allow us to access the Adam class to modify some parameters\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model\nfrom keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting\n",
      "execution_count": 49,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6e6379386e44afc69bee8895a52da22199e888fb"
      },
      "cell_type": "code",
      "source": "# select how many folds will be created\nN_SPLITS = 5\n# it is just a constant with the measurements data size\nsample_size = 800000",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c3340ee96becb5ca8f075d9c44b7df383ddba5ee"
      },
      "cell_type": "code",
      "source": "# It is the official metric used in this competition\n# below is the declaration of a function used inside the keras model, calculation with K (keras backend / thensorflow)\ndef matthews_correlation(y_true, y_pred):\n    '''Calculates the Matthews correlation coefficient measure for quality\n    of binary classification problems.\n    '''\n    y_pred = tf.convert_to_tensor(y_pred, np.float32)\n    y_true = tf.convert_to_tensor(y_true, np.float32)\n    \n    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n    y_pred_neg = 1 - y_pred_pos\n\n    y_pos = K.round(K.clip(y_true, 0, 1))\n    y_neg = 1 - y_pos\n\n    tp = K.sum(y_pos * y_pred_pos)\n    tn = K.sum(y_neg * y_pred_neg)\n\n    fp = K.sum(y_neg * y_pred_pos)\n    fn = K.sum(y_pos * y_pred_neg)\n\n    numerator = (tp * tn - fp * fn)\n    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n\n    return numerator / (denominator + K.epsilon())",
      "execution_count": 50,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eda7ea366117d1ce8e5fce69e5bba333821d8b48"
      },
      "cell_type": "code",
      "source": "# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# just load train data\ndf_train = pd.read_csv('../input/metadata_train.csv')\n# set index, it makes the data access much faster\ndf_train = df_train.set_index(['id_measurement', 'phase'])\ndf_train.head()",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": "                      signal_id  target\nid_measurement phase                   \n0              0              0       0\n               1              1       0\n               2              2       0\n1              0              3       1\n               1              4       1",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>signal_id</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id_measurement</th>\n      <th>phase</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">0</th>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">1</th>\n      <th>0</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "26df6c7fbfecd537404866faec13d1238ae3ebc6"
      },
      "cell_type": "code",
      "source": "# in other notebook I have extracted the min and max values from the train data, the measurements\nmax_num = 127\nmin_num = -128",
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7b0717b14bcfcba1f48d33c8161ae51c778687af"
      },
      "cell_type": "code",
      "source": "# This function standardize the data from (-128 to 127) to (-1 to 1)\n# Theoretically it helps in the NN Model training, but I didn't tested without it\ndef min_max_transf(ts, min_data, max_data, range_needed=(-1,1)):\n    if min_data < 0:\n        ts_std = (ts + abs(min_data)) / (max_data + abs(min_data))\n    else:\n        ts_std = (ts - min_data) / (max_data - min_data)\n    if range_needed[0] < 0:    \n        return ts_std * (range_needed[1] + abs(range_needed[0])) + range_needed[0]\n    else:\n        return ts_std * (range_needed[1] - range_needed[0]) + range_needed[0]",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6137bbbe75c3a1509a5f98e08805dbbd492aa37"
      },
      "cell_type": "code",
      "source": "def transform_ts(ts, n_dim=160, min_max=(-1,1)):\n    # convert data into -1 to 1\n    ts_std = min_max_transf(ts, min_data=min_num, max_data=max_num)\n    # bucket or chunk size, 5000 in this case (800000 / 160)\n    bucket_size = int(sample_size / n_dim)\n    # new_ts will be the container of the new data\n    new_ts = []\n    # this for iteract any chunk/bucket until reach the whole sample_size (800000)\n    for i in range(0, sample_size, bucket_size):\n        # cut each bucket to ts_range\n        ts_range = ts_std[i:i + bucket_size]\n        # calculate each feature\n        mean = ts_range.mean()\n        std = ts_range.std() # standard deviation\n        std_top = mean + std # I have to test it more, but is is like a band\n        std_bot = mean - std\n        # I think that the percentiles are very important, it is like a distribuiton analysis from eath chunk\n        percentil_calc = np.percentile(ts_range, [0, 1, 25, 50, 75, 99, 100]) \n        max_range = percentil_calc[-1] - percentil_calc[0] # this is the amplitude of the chunk\n        relative_percentile = percentil_calc - mean # maybe it could heap to understand the asymmetry\n        new_ts.append(np.concatenate([np.asarray([mean, std, std_top, std_bot, max_range]),percentil_calc, relative_percentile]))\n    return np.asarray(new_ts)",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7460e718a605803f1d9e4fbec61750a0deb02a47"
      },
      "cell_type": "code",
      "source": "# this function take a piece of data and convert using transform_ts(), but it does to each of the 3 phases\n# if we would try to do in one time, could exceed the RAM Memmory\ndef prep_data(start, end):\n\n    praq_train = pq.read_pandas('../input/train.parquet', columns=[str(i) for i in range(start, end)]).to_pandas()\n    X = []\n    y = []\n   \n    for id_measurement in tqdm(df_train.index.levels[0].unique()[int(start/3):int(end/3)]):\n        X_signal = []\n        # for each phase of the signal\n        for phase in [0,1,2]:\n            # extract from df_train both signal_id and target to compose the new data sets\n            signal_id, target = df_train.loc[id_measurement].loc[phase]\n            # but just append the target one time, to not triplicate it\n            if phase == 0:\n                y.append(target)\n            # extract and transform data into sets of features\n            X_signal.append(transform_ts(praq_train[str(signal_id)]))\n        # concatenate all the 3 phases in one matrix\n        X_signal = np.concatenate(X_signal, axis=1)\n        # add the data to X\n        X.append(X_signal)\n    X = np.asarray(X)\n    y = np.asarray(y)\n    return X, y",
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "52dc826ab9ee1dd56c9fb29bd5c1b2d26b5928bf"
      },
      "cell_type": "code",
      "source": "# this code is very simple, divide the total size of the df_train into two sets and process it\nX = []\ny = []\ndef load_all():\n    total_size = len(df_train)\n    for ini, end in [(0, int(total_size/2)), (int(total_size/2), total_size)]:\n        X_temp, y_temp = prep_data(ini, end)\n        X.append(X_temp)\n        y.append(y_temp)\nload_all()\nX = np.concatenate(X)\ny = np.concatenate(y)",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": "100%|██████████| 1452/1452 [07:01<00:00,  3.43it/s]\n100%|██████████| 1452/1452 [07:04<00:00,  3.48it/s]\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51ad0e25b00536de6170168499923d82ae1d735f"
      },
      "cell_type": "code",
      "source": "print(X.shape, y.shape)\n# save data into file, a numpy specific format\nnp.save(\"X.npy\",X)\nnp.save(\"y.npy\",y)",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(2904, 160, 57) (2904,)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "289bc7d1ab8048a60025801b457f8df1d848acbc"
      },
      "cell_type": "code",
      "source": "# This is NN LSTM Model creation\ndef model_lstm(input_shape):\n    # The shape was explained above, must have this order\n    inp = Input(shape=(input_shape[1], input_shape[2],))\n    \n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(inp)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n    x = Attention(input_shape[1])(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dropout(0.25) (x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation])\n    \n    return model",
      "execution_count": 37,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d6f4ca319c383b1b4f671a37c5a324136e7a466"
      },
      "cell_type": "code",
      "source": "# Here is where the training happens\n\n# First, create a set of indexes of the 5 folds\nsplits = list(StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=2019).split(X, y))\npreds_val = []\ny_val = []\n# Then, iteract with each fold\n# If you dont know, enumerate(['a', 'b', 'c']) returns [(0, 'a'), (1, 'b'), (2, 'c')]\nfor idx, (train_idx, val_idx) in enumerate(splits):\n    K.clear_session() # I dont know what it do, but I imagine that it \"clear session\" :)\n    print(\"Beginning fold {}\".format(idx+1))\n    train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n    model = model_lstm(train_X.shape)\n    es = EarlyStopping(monitor='val_matthews_correlation', verbose=2, patience=15)\n    ckpt = ModelCheckpoint('weights_{}.h5'.format(idx), save_best_only=True, save_weights_only=True, verbose=1, monitor='val_matthews_correlation', mode='max')\n    # Train, train, train\n    model.fit(train_X, train_y, batch_size=128, epochs=50, validation_data=[val_X, val_y], callbacks=[ckpt,es])\n    # loads the best weights saved by the checkpoint\n    model.load_weights('weights_{}.h5'.format(idx))\n    # Add the predictions of the validation to the list preds_val\n    preds_val.append(model.predict(val_X, batch_size=512))\n    # and the val true y\n    y_val.append(val_y)\n\n# concatenates all and prints the shape    \npreds_val = np.concatenate(preds_val)[...,0]\ny_val = np.concatenate(y_val)\npreds_val.shape, y_val.shape",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Beginning fold 1\nTrain on 2322 samples, validate on 582 samples\nEpoch 1/50\n2322/2322 [==============================] - 5s 2ms/step - loss: 0.3547 - matthews_correlation: 0.0075 - val_loss: 0.2303 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_0.h5\nEpoch 2/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.2351 - matthews_correlation: 0.0000e+00 - val_loss: 0.2244 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00002: val_matthews_correlation did not improve from 0.00000\nEpoch 3/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.2180 - matthews_correlation: 0.0000e+00 - val_loss: 0.2103 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00003: val_matthews_correlation did not improve from 0.00000\nEpoch 4/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.2149 - matthews_correlation: -0.0014 - val_loss: 0.2022 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00004: val_matthews_correlation did not improve from 0.00000\nEpoch 5/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1948 - matthews_correlation: 0.0147 - val_loss: 0.1864 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00005: val_matthews_correlation did not improve from 0.00000\nEpoch 6/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1888 - matthews_correlation: 0.0000e+00 - val_loss: 0.1555 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00006: val_matthews_correlation did not improve from 0.00000\nEpoch 7/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1634 - matthews_correlation: 0.0392 - val_loss: 0.1269 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00007: val_matthews_correlation did not improve from 0.00000\nEpoch 8/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1278 - matthews_correlation: 0.1330 - val_loss: 0.1051 - val_matthews_correlation: 0.7118\n\nEpoch 00008: val_matthews_correlation improved from 0.00000 to 0.71178, saving model to weights_0.h5\nEpoch 9/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1536 - matthews_correlation: 0.4036 - val_loss: 0.1618 - val_matthews_correlation: 0.6851\n\nEpoch 00009: val_matthews_correlation did not improve from 0.71178\nEpoch 10/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1305 - matthews_correlation: 0.5485 - val_loss: 0.0866 - val_matthews_correlation: 0.7195\n\nEpoch 00010: val_matthews_correlation improved from 0.71178 to 0.71954, saving model to weights_0.h5\nEpoch 11/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1184 - matthews_correlation: 0.6383 - val_loss: 0.0889 - val_matthews_correlation: 0.7591\n\nEpoch 00011: val_matthews_correlation improved from 0.71954 to 0.75913, saving model to weights_0.h5\nEpoch 12/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1042 - matthews_correlation: 0.6553 - val_loss: 0.0838 - val_matthews_correlation: 0.7124\n\nEpoch 00012: val_matthews_correlation did not improve from 0.75913\nEpoch 13/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1077 - matthews_correlation: 0.5886 - val_loss: 0.0880 - val_matthews_correlation: 0.7815\n\nEpoch 00013: val_matthews_correlation improved from 0.75913 to 0.78149, saving model to weights_0.h5\nEpoch 14/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1041 - matthews_correlation: 0.6213 - val_loss: 0.0888 - val_matthews_correlation: 0.8106\n\nEpoch 00014: val_matthews_correlation improved from 0.78149 to 0.81064, saving model to weights_0.h5\nEpoch 15/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1108 - matthews_correlation: 0.6033 - val_loss: 0.0800 - val_matthews_correlation: 0.7757\n\nEpoch 00015: val_matthews_correlation did not improve from 0.81064\nEpoch 16/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1040 - matthews_correlation: 0.6518 - val_loss: 0.0886 - val_matthews_correlation: 0.6526\n\nEpoch 00016: val_matthews_correlation did not improve from 0.81064\nEpoch 17/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1093 - matthews_correlation: 0.5536 - val_loss: 0.0951 - val_matthews_correlation: 0.7429\n\nEpoch 00017: val_matthews_correlation did not improve from 0.81064\nEpoch 18/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1055 - matthews_correlation: 0.6842 - val_loss: 0.0826 - val_matthews_correlation: 0.7904\n\nEpoch 00018: val_matthews_correlation did not improve from 0.81064\nEpoch 19/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1047 - matthews_correlation: 0.6337 - val_loss: 0.0783 - val_matthews_correlation: 0.7044\n\nEpoch 00019: val_matthews_correlation did not improve from 0.81064\nEpoch 20/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1035 - matthews_correlation: 0.6269 - val_loss: 0.0854 - val_matthews_correlation: 0.7546\n\nEpoch 00020: val_matthews_correlation did not improve from 0.81064\nEpoch 21/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1048 - matthews_correlation: 0.5653 - val_loss: 0.0866 - val_matthews_correlation: 0.7080\n\nEpoch 00021: val_matthews_correlation did not improve from 0.81064\nEpoch 22/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1037 - matthews_correlation: 0.6074 - val_loss: 0.0804 - val_matthews_correlation: 0.7495\n\nEpoch 00022: val_matthews_correlation did not improve from 0.81064\nEpoch 23/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1119 - matthews_correlation: 0.6398 - val_loss: 0.0791 - val_matthews_correlation: 0.7722\n\nEpoch 00023: val_matthews_correlation did not improve from 0.81064\nEpoch 24/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.0994 - matthews_correlation: 0.6636 - val_loss: 0.0803 - val_matthews_correlation: 0.7864\n\nEpoch 00024: val_matthews_correlation did not improve from 0.81064\nEpoch 25/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1099 - matthews_correlation: 0.6578 - val_loss: 0.0790 - val_matthews_correlation: 0.7387\n\nEpoch 00025: val_matthews_correlation did not improve from 0.81064\nEpoch 26/50\n2322/2322 [==============================] - 3s 1ms/step - loss: 0.1056 - matthews_correlation: 0.6593 - val_loss: 0.0833 - val_matthews_correlation: 0.7511\n\nEpoch 00026: val_matthews_correlation did not improve from 0.81064\nEpoch 00026: early stopping\nBeginning fold 2\nTrain on 2323 samples, validate on 581 samples\nEpoch 1/50\n2323/2323 [==============================] - 4s 2ms/step - loss: 0.3422 - matthews_correlation: -0.0059 - val_loss: 0.2310 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_1.h5\nEpoch 2/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.2320 - matthews_correlation: 0.0000e+00 - val_loss: 0.2344 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00002: val_matthews_correlation did not improve from 0.00000\nEpoch 3/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.2268 - matthews_correlation: 0.0000e+00 - val_loss: 0.2260 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00003: val_matthews_correlation did not improve from 0.00000\nEpoch 4/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.2178 - matthews_correlation: 0.0000e+00 - val_loss: 0.2240 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00004: val_matthews_correlation did not improve from 0.00000\nEpoch 5/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.2143 - matthews_correlation: 0.0000e+00 - val_loss: 0.2084 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00005: val_matthews_correlation did not improve from 0.00000\nEpoch 6/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1926 - matthews_correlation: 0.2389 - val_loss: 0.2031 - val_matthews_correlation: 0.0219\n\nEpoch 00006: val_matthews_correlation improved from 0.00000 to 0.02191, saving model to weights_1.h5\nEpoch 7/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1749 - matthews_correlation: 0.2069 - val_loss: 0.1626 - val_matthews_correlation: 0.2335\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "\nEpoch 00007: val_matthews_correlation improved from 0.02191 to 0.23348, saving model to weights_1.h5\nEpoch 8/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1369 - matthews_correlation: 0.3547 - val_loss: 0.1266 - val_matthews_correlation: 0.5305\n\nEpoch 00008: val_matthews_correlation improved from 0.23348 to 0.53051, saving model to weights_1.h5\nEpoch 9/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1395 - matthews_correlation: 0.5536 - val_loss: 0.1305 - val_matthews_correlation: 0.4950\n\nEpoch 00009: val_matthews_correlation did not improve from 0.53051\nEpoch 10/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1143 - matthews_correlation: 0.6322 - val_loss: 0.1164 - val_matthews_correlation: 0.6053\n\nEpoch 00010: val_matthews_correlation improved from 0.53051 to 0.60526, saving model to weights_1.h5\nEpoch 11/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1128 - matthews_correlation: 0.6451 - val_loss: 0.1119 - val_matthews_correlation: 0.6051\n\nEpoch 00011: val_matthews_correlation did not improve from 0.60526\nEpoch 12/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1085 - matthews_correlation: 0.6381 - val_loss: 0.1113 - val_matthews_correlation: 0.6208\n\nEpoch 00012: val_matthews_correlation improved from 0.60526 to 0.62081, saving model to weights_1.h5\nEpoch 13/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1089 - matthews_correlation: 0.6467 - val_loss: 0.1127 - val_matthews_correlation: 0.6306\n\nEpoch 00013: val_matthews_correlation improved from 0.62081 to 0.63056, saving model to weights_1.h5\nEpoch 14/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1027 - matthews_correlation: 0.6519 - val_loss: 0.1067 - val_matthews_correlation: 0.6519\n\nEpoch 00014: val_matthews_correlation improved from 0.63056 to 0.65185, saving model to weights_1.h5\nEpoch 15/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1014 - matthews_correlation: 0.6203 - val_loss: 0.1041 - val_matthews_correlation: 0.6385\n\nEpoch 00015: val_matthews_correlation did not improve from 0.65185\nEpoch 16/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1008 - matthews_correlation: 0.6339 - val_loss: 0.1030 - val_matthews_correlation: 0.6385\n\nEpoch 00016: val_matthews_correlation did not improve from 0.65185\nEpoch 17/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.0943 - matthews_correlation: 0.6927 - val_loss: 0.1042 - val_matthews_correlation: 0.6676\n\nEpoch 00017: val_matthews_correlation improved from 0.65185 to 0.66761, saving model to weights_1.h5\nEpoch 18/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1046 - matthews_correlation: 0.6271 - val_loss: 0.1048 - val_matthews_correlation: 0.6533\n\nEpoch 00018: val_matthews_correlation did not improve from 0.66761\nEpoch 19/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1071 - matthews_correlation: 0.6613 - val_loss: 0.1028 - val_matthews_correlation: 0.6385\n\nEpoch 00019: val_matthews_correlation did not improve from 0.66761\nEpoch 20/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1069 - matthews_correlation: 0.6366 - val_loss: 0.1089 - val_matthews_correlation: 0.6387\n\nEpoch 00020: val_matthews_correlation did not improve from 0.66761\nEpoch 21/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1035 - matthews_correlation: 0.6424 - val_loss: 0.0990 - val_matthews_correlation: 0.6691\n\nEpoch 00021: val_matthews_correlation improved from 0.66761 to 0.66910, saving model to weights_1.h5\nEpoch 22/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.0982 - matthews_correlation: 0.6751 - val_loss: 0.0939 - val_matthews_correlation: 0.6688\n\nEpoch 00022: val_matthews_correlation did not improve from 0.66910\nEpoch 23/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.0977 - matthews_correlation: 0.6752 - val_loss: 0.0970 - val_matthews_correlation: 0.6526\n\nEpoch 00023: val_matthews_correlation did not improve from 0.66910\nEpoch 24/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.0929 - matthews_correlation: 0.6806 - val_loss: 0.0979 - val_matthews_correlation: 0.6564\n\nEpoch 00024: val_matthews_correlation did not improve from 0.66910\nEpoch 25/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.0933 - matthews_correlation: 0.6852 - val_loss: 0.0963 - val_matthews_correlation: 0.7000\n\nEpoch 00025: val_matthews_correlation improved from 0.66910 to 0.69997, saving model to weights_1.h5\nEpoch 26/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1005 - matthews_correlation: 0.6662 - val_loss: 0.1000 - val_matthews_correlation: 0.6638\n\nEpoch 00026: val_matthews_correlation did not improve from 0.69997\nEpoch 00026: early stopping\nBeginning fold 3\nTrain on 2323 samples, validate on 581 samples\nEpoch 1/50\n2323/2323 [==============================] - 5s 2ms/step - loss: 0.3564 - matthews_correlation: 0.0039 - val_loss: 0.2359 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_2.h5\nEpoch 2/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.2315 - matthews_correlation: 0.0000e+00 - val_loss: 0.2229 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00002: val_matthews_correlation did not improve from 0.00000\nEpoch 3/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.2316 - matthews_correlation: 0.0000e+00 - val_loss: 0.2192 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00003: val_matthews_correlation did not improve from 0.00000\nEpoch 4/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.2240 - matthews_correlation: 0.0000e+00 - val_loss: 0.2080 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00004: val_matthews_correlation did not improve from 0.00000\nEpoch 5/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.2081 - matthews_correlation: 0.0107 - val_loss: 0.1933 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00005: val_matthews_correlation did not improve from 0.00000\nEpoch 6/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1922 - matthews_correlation: 0.0940 - val_loss: 0.1757 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00006: val_matthews_correlation did not improve from 0.00000\nEpoch 7/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1715 - matthews_correlation: 0.1320 - val_loss: 0.1992 - val_matthews_correlation: 0.2472\n\nEpoch 00007: val_matthews_correlation improved from 0.00000 to 0.24720, saving model to weights_2.h5\nEpoch 8/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1880 - matthews_correlation: 0.1114 - val_loss: 0.1727 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00008: val_matthews_correlation did not improve from 0.24720\nEpoch 9/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1666 - matthews_correlation: -0.0037 - val_loss: 0.1459 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00009: val_matthews_correlation did not improve from 0.24720\nEpoch 10/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1672 - matthews_correlation: 0.0432 - val_loss: 0.1630 - val_matthews_correlation: 0.3272\n\nEpoch 00010: val_matthews_correlation improved from 0.24720 to 0.32724, saving model to weights_2.h5\nEpoch 11/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1373 - matthews_correlation: 0.3104 - val_loss: 0.1187 - val_matthews_correlation: 0.4897\n\nEpoch 00011: val_matthews_correlation improved from 0.32724 to 0.48972, saving model to weights_2.h5\nEpoch 12/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1150 - matthews_correlation: 0.5824 - val_loss: 0.1332 - val_matthews_correlation: 0.4126\n\nEpoch 00012: val_matthews_correlation did not improve from 0.48972\nEpoch 13/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1145 - matthews_correlation: 0.6147 - val_loss: 0.1145 - val_matthews_correlation: 0.5179\n\nEpoch 00013: val_matthews_correlation improved from 0.48972 to 0.51790, saving model to weights_2.h5\nEpoch 14/50\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "2323/2323 [==============================] - 3s 1ms/step - loss: 0.1000 - matthews_correlation: 0.6425 - val_loss: 0.1188 - val_matthews_correlation: 0.6101\n\nEpoch 00014: val_matthews_correlation improved from 0.51790 to 0.61010, saving model to weights_2.h5\nEpoch 15/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1067 - matthews_correlation: 0.6341 - val_loss: 0.1181 - val_matthews_correlation: 0.6477\n\nEpoch 00015: val_matthews_correlation improved from 0.61010 to 0.64769, saving model to weights_2.h5\nEpoch 16/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1008 - matthews_correlation: 0.7057 - val_loss: 0.1234 - val_matthews_correlation: 0.4433\n\nEpoch 00016: val_matthews_correlation did not improve from 0.64769\nEpoch 17/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1063 - matthews_correlation: 0.5947 - val_loss: 0.1130 - val_matthews_correlation: 0.6327\n\nEpoch 00017: val_matthews_correlation did not improve from 0.64769\nEpoch 18/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1046 - matthews_correlation: 0.6113 - val_loss: 0.1098 - val_matthews_correlation: 0.6219\n\nEpoch 00018: val_matthews_correlation did not improve from 0.64769\nEpoch 19/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.0992 - matthews_correlation: 0.6726 - val_loss: 0.1120 - val_matthews_correlation: 0.5144\n\nEpoch 00019: val_matthews_correlation did not improve from 0.64769\nEpoch 20/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.0950 - matthews_correlation: 0.6275 - val_loss: 0.1173 - val_matthews_correlation: 0.5959\n\nEpoch 00020: val_matthews_correlation did not improve from 0.64769\nEpoch 21/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.0955 - matthews_correlation: 0.6672 - val_loss: 0.1170 - val_matthews_correlation: 0.5237\n\nEpoch 00021: val_matthews_correlation did not improve from 0.64769\nEpoch 22/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.1005 - matthews_correlation: 0.6253 - val_loss: 0.1157 - val_matthews_correlation: 0.5233\n\nEpoch 00022: val_matthews_correlation did not improve from 0.64769\nEpoch 23/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.0975 - matthews_correlation: 0.6590 - val_loss: 0.1212 - val_matthews_correlation: 0.3693\n\nEpoch 00023: val_matthews_correlation did not improve from 0.64769\nEpoch 24/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.0932 - matthews_correlation: 0.6780 - val_loss: 0.1161 - val_matthews_correlation: 0.6588\n\nEpoch 00024: val_matthews_correlation improved from 0.64769 to 0.65876, saving model to weights_2.h5\nEpoch 25/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.0939 - matthews_correlation: 0.7081 - val_loss: 0.1131 - val_matthews_correlation: 0.6067\n\nEpoch 00025: val_matthews_correlation did not improve from 0.65876\nEpoch 26/50\n2323/2323 [==============================] - 3s 1ms/step - loss: 0.0950 - matthews_correlation: 0.6921 - val_loss: 0.1146 - val_matthews_correlation: 0.5367\n\nEpoch 00026: val_matthews_correlation did not improve from 0.65876\nEpoch 00026: early stopping\nBeginning fold 4\nTrain on 2324 samples, validate on 580 samples\nEpoch 1/50\n2324/2324 [==============================] - 4s 2ms/step - loss: 0.3540 - matthews_correlation: -0.0053 - val_loss: 0.2366 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_3.h5\nEpoch 2/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.2389 - matthews_correlation: 0.0000e+00 - val_loss: 0.2211 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00002: val_matthews_correlation did not improve from 0.00000\nEpoch 3/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.2289 - matthews_correlation: 0.0000e+00 - val_loss: 0.2114 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00003: val_matthews_correlation did not improve from 0.00000\nEpoch 4/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.2197 - matthews_correlation: 0.0000e+00 - val_loss: 0.1985 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00004: val_matthews_correlation did not improve from 0.00000\nEpoch 5/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.2013 - matthews_correlation: 0.0000e+00 - val_loss: 0.2011 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00005: val_matthews_correlation did not improve from 0.00000\nEpoch 6/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1908 - matthews_correlation: 0.0000e+00 - val_loss: 0.1761 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00006: val_matthews_correlation did not improve from 0.00000\nEpoch 7/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1612 - matthews_correlation: 0.0609 - val_loss: 0.1239 - val_matthews_correlation: 0.1659\n\nEpoch 00007: val_matthews_correlation improved from 0.00000 to 0.16593, saving model to weights_3.h5\nEpoch 8/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1330 - matthews_correlation: 0.4490 - val_loss: 0.1243 - val_matthews_correlation: 0.5724\n\nEpoch 00008: val_matthews_correlation improved from 0.16593 to 0.57242, saving model to weights_3.h5\nEpoch 9/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1431 - matthews_correlation: 0.5214 - val_loss: 0.1381 - val_matthews_correlation: 0.1585\n\nEpoch 00009: val_matthews_correlation did not improve from 0.57242\nEpoch 10/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1161 - matthews_correlation: 0.4885 - val_loss: 0.1125 - val_matthews_correlation: 0.6503\n\nEpoch 00010: val_matthews_correlation improved from 0.57242 to 0.65032, saving model to weights_3.h5\nEpoch 11/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1079 - matthews_correlation: 0.6355 - val_loss: 0.1106 - val_matthews_correlation: 0.5584\n\nEpoch 00011: val_matthews_correlation did not improve from 0.65032\nEpoch 12/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1067 - matthews_correlation: 0.6438 - val_loss: 0.1125 - val_matthews_correlation: 0.5584\n\nEpoch 00012: val_matthews_correlation did not improve from 0.65032\nEpoch 13/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1018 - matthews_correlation: 0.6391 - val_loss: 0.1375 - val_matthews_correlation: 0.6341\n\nEpoch 00013: val_matthews_correlation did not improve from 0.65032\nEpoch 14/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1121 - matthews_correlation: 0.6022 - val_loss: 0.1153 - val_matthews_correlation: 0.5189\n\nEpoch 00014: val_matthews_correlation did not improve from 0.65032\nEpoch 15/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1035 - matthews_correlation: 0.6604 - val_loss: 0.1170 - val_matthews_correlation: 0.5755\n\nEpoch 00015: val_matthews_correlation did not improve from 0.65032\nEpoch 16/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.0982 - matthews_correlation: 0.6670 - val_loss: 0.1225 - val_matthews_correlation: 0.4927\n\nEpoch 00016: val_matthews_correlation did not improve from 0.65032\nEpoch 17/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1010 - matthews_correlation: 0.6611 - val_loss: 0.1435 - val_matthews_correlation: 0.5352\n\nEpoch 00017: val_matthews_correlation did not improve from 0.65032\nEpoch 18/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1074 - matthews_correlation: 0.6540 - val_loss: 0.1096 - val_matthews_correlation: 0.5892\n\nEpoch 00018: val_matthews_correlation did not improve from 0.65032\nEpoch 19/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1072 - matthews_correlation: 0.6329 - val_loss: 0.1051 - val_matthews_correlation: 0.6206\n\nEpoch 00019: val_matthews_correlation did not improve from 0.65032\nEpoch 20/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.0964 - matthews_correlation: 0.6893 - val_loss: 0.1202 - val_matthews_correlation: 0.5524\n\nEpoch 00020: val_matthews_correlation did not improve from 0.65032\nEpoch 21/50\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "2324/2324 [==============================] - 3s 1ms/step - loss: 0.1073 - matthews_correlation: 0.6178 - val_loss: 0.1100 - val_matthews_correlation: 0.5676\n\nEpoch 00021: val_matthews_correlation did not improve from 0.65032\nEpoch 22/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.0973 - matthews_correlation: 0.6740 - val_loss: 0.0999 - val_matthews_correlation: 0.6646\n\nEpoch 00022: val_matthews_correlation improved from 0.65032 to 0.66455, saving model to weights_3.h5\nEpoch 23/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.0970 - matthews_correlation: 0.6350 - val_loss: 0.1047 - val_matthews_correlation: 0.6418\n\nEpoch 00023: val_matthews_correlation did not improve from 0.66455\nEpoch 24/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.0960 - matthews_correlation: 0.6655 - val_loss: 0.1006 - val_matthews_correlation: 0.5133\n\nEpoch 00024: val_matthews_correlation did not improve from 0.66455\nEpoch 25/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.0947 - matthews_correlation: 0.6078 - val_loss: 0.0990 - val_matthews_correlation: 0.6933\n\nEpoch 00025: val_matthews_correlation improved from 0.66455 to 0.69333, saving model to weights_3.h5\nEpoch 26/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.0957 - matthews_correlation: 0.6709 - val_loss: 0.1061 - val_matthews_correlation: 0.4927\n\nEpoch 00026: val_matthews_correlation did not improve from 0.69333\nEpoch 00026: early stopping\nBeginning fold 5\nTrain on 2324 samples, validate on 580 samples\nEpoch 1/50\n2324/2324 [==============================] - 4s 2ms/step - loss: 0.3506 - matthews_correlation: 0.0025 - val_loss: 0.2323 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_4.h5\nEpoch 2/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.2410 - matthews_correlation: 0.0000e+00 - val_loss: 0.2224 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00002: val_matthews_correlation did not improve from 0.00000\nEpoch 3/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.2285 - matthews_correlation: 0.0000e+00 - val_loss: 0.2050 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00003: val_matthews_correlation did not improve from 0.00000\nEpoch 4/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.2094 - matthews_correlation: 0.0000e+00 - val_loss: 0.1961 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00004: val_matthews_correlation did not improve from 0.00000\nEpoch 5/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1897 - matthews_correlation: 0.0000e+00 - val_loss: 0.1890 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00005: val_matthews_correlation did not improve from 0.00000\nEpoch 6/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1819 - matthews_correlation: 0.0000e+00 - val_loss: 0.1780 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00006: val_matthews_correlation did not improve from 0.00000\nEpoch 7/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1820 - matthews_correlation: 0.0000e+00 - val_loss: 0.2027 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00007: val_matthews_correlation did not improve from 0.00000\nEpoch 8/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1913 - matthews_correlation: 0.0553 - val_loss: 0.2003 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00008: val_matthews_correlation did not improve from 0.00000\nEpoch 9/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.2089 - matthews_correlation: 0.0000e+00 - val_loss: 0.2032 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00009: val_matthews_correlation did not improve from 0.00000\nEpoch 10/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1866 - matthews_correlation: 0.0000e+00 - val_loss: 0.1637 - val_matthews_correlation: 0.0000e+00\n\nEpoch 00010: val_matthews_correlation did not improve from 0.00000\nEpoch 11/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1481 - matthews_correlation: 0.0803 - val_loss: 0.1426 - val_matthews_correlation: 0.4838\n\nEpoch 00011: val_matthews_correlation improved from 0.00000 to 0.48376, saving model to weights_4.h5\nEpoch 12/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1304 - matthews_correlation: 0.4282 - val_loss: 0.1228 - val_matthews_correlation: 0.4560\n\nEpoch 00012: val_matthews_correlation did not improve from 0.48376\nEpoch 13/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1259 - matthews_correlation: 0.4364 - val_loss: 0.1430 - val_matthews_correlation: 0.5189\n\nEpoch 00013: val_matthews_correlation improved from 0.48376 to 0.51893, saving model to weights_4.h5\nEpoch 14/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1237 - matthews_correlation: 0.4803 - val_loss: 0.1267 - val_matthews_correlation: 0.4675\n\nEpoch 00014: val_matthews_correlation did not improve from 0.51893\nEpoch 15/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1200 - matthews_correlation: 0.5457 - val_loss: 0.1237 - val_matthews_correlation: 0.5440\n\nEpoch 00015: val_matthews_correlation improved from 0.51893 to 0.54398, saving model to weights_4.h5\nEpoch 16/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1247 - matthews_correlation: 0.5441 - val_loss: 0.1258 - val_matthews_correlation: 0.6219\n\nEpoch 00016: val_matthews_correlation improved from 0.54398 to 0.62195, saving model to weights_4.h5\nEpoch 17/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1203 - matthews_correlation: 0.5617 - val_loss: 0.1170 - val_matthews_correlation: 0.5777\n\nEpoch 00017: val_matthews_correlation did not improve from 0.62195\nEpoch 18/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1053 - matthews_correlation: 0.6363 - val_loss: 0.1124 - val_matthews_correlation: 0.6077\n\nEpoch 00018: val_matthews_correlation did not improve from 0.62195\nEpoch 19/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1016 - matthews_correlation: 0.6225 - val_loss: 0.1083 - val_matthews_correlation: 0.6193\n\nEpoch 00019: val_matthews_correlation did not improve from 0.62195\nEpoch 20/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1040 - matthews_correlation: 0.6302 - val_loss: 0.1070 - val_matthews_correlation: 0.6209\n\nEpoch 00020: val_matthews_correlation did not improve from 0.62195\nEpoch 21/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1035 - matthews_correlation: 0.6567 - val_loss: 0.1220 - val_matthews_correlation: 0.5887\n\nEpoch 00021: val_matthews_correlation did not improve from 0.62195\nEpoch 22/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.0953 - matthews_correlation: 0.6296 - val_loss: 0.1063 - val_matthews_correlation: 0.6133\n\nEpoch 00022: val_matthews_correlation did not improve from 0.62195\nEpoch 23/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1021 - matthews_correlation: 0.6617 - val_loss: 0.1056 - val_matthews_correlation: 0.5623\n\nEpoch 00023: val_matthews_correlation did not improve from 0.62195\nEpoch 24/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.1050 - matthews_correlation: 0.5997 - val_loss: 0.1098 - val_matthews_correlation: 0.6212\n\nEpoch 00024: val_matthews_correlation did not improve from 0.62195\nEpoch 25/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.0979 - matthews_correlation: 0.6203 - val_loss: 0.1082 - val_matthews_correlation: 0.6162\n\nEpoch 00025: val_matthews_correlation did not improve from 0.62195\nEpoch 26/50\n2324/2324 [==============================] - 3s 1ms/step - loss: 0.0959 - matthews_correlation: 0.6230 - val_loss: 0.1315 - val_matthews_correlation: 0.3225\n\nEpoch 00026: val_matthews_correlation did not improve from 0.62195\nEpoch 00026: early stopping\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 40,
          "data": {
            "text/plain": "((2904,), (2904,))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d28151fd0be9fd9762f3f55e307d82f89bfbd291"
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\n\ndef threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in tqdm([i * 0.01 for i in range(100)]):\n        score = K.eval(matthews_correlation(y_true.astype(np.float64), (y_proba > threshold).astype(np.float64)))\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'matthews_correlation': best_score}\n    return search_result",
      "execution_count": 53,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c252624065ee8551ae88a2c10b343e4fee3ecc80"
      },
      "cell_type": "code",
      "source": "best_threshold = threshold_search(y_val, preds_val)['threshold']",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": "100%|██████████| 100/100 [00:25<00:00,  2.62it/s]\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8fc5b3dceb6f9a2b3882af4342db975c4f487062"
      },
      "cell_type": "code",
      "source": "best_threshold",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 55,
          "data": {
            "text/plain": "0.5"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ae9bd3fa9d8c0781c0708846bb7f2a9f9e6cbd3c"
      },
      "cell_type": "code",
      "source": "meta_test = pd.read_csv('../input/metadata_test.csv')",
      "execution_count": 56,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3eb186d032f79c99ffba05dd1a7fabb77e13cec5"
      },
      "cell_type": "code",
      "source": "meta_test = meta_test.set_index(['signal_id'])\nmeta_test.head()",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 57,
          "data": {
            "text/plain": "           id_measurement  phase\nsignal_id                       \n8712                 2904      0\n8713                 2904      1\n8714                 2904      2\n8715                 2905      0\n8716                 2905      1",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_measurement</th>\n      <th>phase</th>\n    </tr>\n    <tr>\n      <th>signal_id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8712</th>\n      <td>2904</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8713</th>\n      <td>2904</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8714</th>\n      <td>2904</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8715</th>\n      <td>2905</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8716</th>\n      <td>2905</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f8e94387f625bff0a9a6289e1ee038908bc5856"
      },
      "cell_type": "code",
      "source": "first_sig = meta_test.index[0]\nn_parts = 10\nmax_line = len(meta_test)\npart_size = int(max_line / n_parts)\nlast_part = max_line % n_parts\nprint(first_sig, n_parts, max_line, part_size, last_part, n_parts * part_size + last_part)\n# Here we create a list of lists with start index and end index for each of the 10 parts and one for the last partial part\nstart_end = [[x, x+part_size] for x in range(first_sig, max_line + first_sig, part_size)]\nstart_end = start_end[:-1] + [[start_end[-1][0], start_end[-1][0] + last_part]]\nprint(start_end)\nX_test = []\n# now, very like we did above with the train data, we convert the test data part by part\n# transforming the 3 phases 800000 measurement in matrix (160,57)\nfor start, end in start_end:\n    subset_test = pq.read_pandas('../input/test.parquet', columns=[str(i) for i in range(start, end)]).to_pandas()\n    for i in tqdm(subset_test.columns):\n        id_measurement, phase = meta_test.loc[int(i)]\n        subset_test_col = subset_test[i]\n        subset_trans = transform_ts(subset_test_col)\n        X_test.append([i, id_measurement, phase, subset_trans])",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": "8712 10 20337 2033 7 20337\n[[8712, 10745], [10745, 12778], [12778, 14811], [14811, 16844], [16844, 18877], [18877, 20910], [20910, 22943], [22943, 24976], [24976, 27009], [27009, 29042], [29042, 29049]]\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "100%|██████████| 2033/2033 [03:17<00:00, 10.32it/s]\n100%|██████████| 2033/2033 [03:18<00:00, 10.27it/s]\n100%|██████████| 2033/2033 [03:17<00:00, 10.31it/s]\n100%|██████████| 2033/2033 [03:16<00:00,  9.87it/s]\n100%|██████████| 2033/2033 [03:15<00:00, 10.35it/s]\n100%|██████████| 2033/2033 [03:15<00:00, 10.35it/s]\n100%|██████████| 2033/2033 [03:16<00:00, 10.35it/s]\n100%|██████████| 2033/2033 [03:16<00:00, 10.46it/s]\n100%|██████████| 2033/2033 [03:16<00:00, 10.60it/s]\n100%|██████████| 2033/2033 [03:17<00:00, 10.41it/s]\n100%|██████████| 7/7 [00:00<00:00,  8.88it/s]\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af9aa6b2b8f8a2beda1a02ff998e3072fcad8d06"
      },
      "cell_type": "code",
      "source": "X_test_input = np.asarray([np.concatenate([X_test[i][3],X_test[i+1][3], X_test[i+2][3]], axis=1) for i in range(0,len(X_test), 3)])\nnp.save(\"X_test.npy\",X_test_input)\nX_test_input.shape",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 59,
          "data": {
            "text/plain": "(6779, 160, 57)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cfd265d3e07c4cc1679d2c4d55fe7de631c813e7"
      },
      "cell_type": "code",
      "source": "submission = pd.read_csv('../input/sample_submission.csv')\nprint(len(submission))\nsubmission.head()",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": "20337\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 60,
          "data": {
            "text/plain": "   signal_id  target\n0       8712       0\n1       8713       0\n2       8714       0\n3       8715       0\n4       8716       0",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>signal_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8712</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8713</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8714</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8715</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8716</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f7342296138f6bfd3e9cedd029e1035de3b98fc"
      },
      "cell_type": "code",
      "source": "preds_test = []\nfor i in range(N_SPLITS):\n    model.load_weights('weights_{}.h5'.format(i))\n    pred = model.predict(X_test_input, batch_size=300, verbose=1)\n    pred_3 = []\n    for pred_scalar in pred:\n        for i in range(3):\n            pred_3.append(pred_scalar)\n    preds_test.append(pred_3)\n",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": "6779/6779 [==============================] - 2s 285us/step\n6779/6779 [==============================] - 2s 262us/step\n6779/6779 [==============================] - 2s 266us/step\n6779/6779 [==============================] - 2s 264us/step\n6779/6779 [==============================] - 2s 269us/step\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4b212616b85a0c704ad8dd11d2ea88c818a4d2a2"
      },
      "cell_type": "code",
      "source": "preds_test = (np.squeeze(np.mean(preds_test, axis=0)) > best_threshold).astype(np.int)\npreds_test.shape",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 63,
          "data": {
            "text/plain": "(20337,)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "347a76deca88ff3834c2b372391ada6b30a6ae08"
      },
      "cell_type": "code",
      "source": "submission['target'] = preds_test\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 64,
          "data": {
            "text/plain": "   signal_id  target\n0       8712       0\n1       8713       0\n2       8714       0\n3       8715       0\n4       8716       0",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>signal_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8712</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8713</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8714</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8715</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8716</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bcedf2fa98394bcfd559f66b847df8f1e5a94ff3"
      },
      "cell_type": "code",
      "source": "preds_test_2 = (np.squeeze(np.max(preds_test, axis=0)) > best_threshold).astype(np.int)\npreds_test.shape",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 65,
          "data": {
            "text/plain": "(20337,)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5fc1a689c814349babfe313816a64084c6a6096"
      },
      "cell_type": "code",
      "source": "submission['target'] = preds_test\nsubmission.to_csv('submission_2.csv', index=False)\nsubmission.head()",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 66,
          "data": {
            "text/plain": "   signal_id  target\n0       8712       0\n1       8713       0\n2       8714       0\n3       8715       0\n4       8716       0",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>signal_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8712</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8713</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8714</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8715</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8716</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}